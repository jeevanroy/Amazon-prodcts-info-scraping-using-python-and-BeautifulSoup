# Amazon-prodcts-info-scraping-using-python-and-BeautifulSoup
This is an amazing exercise to work on web scraping using beautifulSoup.


## Instructions:

I have written the code in Macbook. If you're running the files in systems having other than macOS, keep an eye on the directories. Make sure you pass valid directories.

unzip the webpages file. This folder contains 40 files which includes 20 .html files and 20 folders with corresponding page items like images and other files.

Make sure you keep all the files, i.e., part1.py, part2.ipynb, webpages folder in a directory with name "00Analystt.ai".

run the part1.py file in 'atom' or any other IDE you prefer. Running this code will create the 'part1' '.csv' file with name "Part 1_20_pages_products.csv".

run the part2.ipynb in 'jupyter notebook'. The reason why I used jupyternotebook was I wanted to use the url links in the part1.csv file. Running this code successfully creates 'part2' '.csv' file with name "Part_2_products_info.csv".


## Challenges I faced:

A few pages have different layout. 

Some elements I wanted to extract doesn't have any grip unlike others have classes and ids' in their tags.

Making sure my 'user Agent' in 'headers' is correct.

