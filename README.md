# Amazon-products-info-scraping-using-python-and-BeautifulSoup
It is an excellent exercise to work on web scraping using beautiful soup.


## Instructions:

Macbook is used to develop the code. Running the files in systems other than macOS requires attention to the directories and paths provided.

Unzip the webpages file. The resulting folder contains 40 files, including 20 .html files and 20 folders with corresponding page items like images and other files.

Make sure all the files, i.e., part1.py, part2.ipynb, webpages folder, is in a directory(folder) named "00Analystt.ai".

Run the part1.py file in 'atom' or any other IDE you prefer. Running this code will create the 'part1' '.csv' file named "Part 1_20_pages_products.csv".

Run the part2.ipynb in 'Jupiter notebook'. Use Jupiter notebook to utilise the URL links in the part1.csv file. Running this code creates a 'part2' '.csv' file named "Part_2_products_info.csv".


## Challenges I faced:

A few pages have different layouts. 

Some elements I wanted to extract do not have any grip, unlike others with classes and ids' in their tags.

Make sure 'user Agent' in 'headers' is correct.

